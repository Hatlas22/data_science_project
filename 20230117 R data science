dataset <- read.table("Live_20210128.csv", header = TRUE, sep=",", row.names =)

#1
dim(dataset)

#2
is.na(dataset)
within(dataset, rm("Column1", "Column2", "Column3", "Column4"))
dataset = dataset[1:12]

#3
summary(dataset[1, 4:12])

#4
#Si 2 variables sont parfaitement corrélées, pas approprié de les inclure toutes les 2
#car contiennent alors les mêmes infoset ça ne contribue donc pas à l'augmentation de la variance expliquée, que la PCA cherche à maximiser.
#Vaut mieux garder que l'info la plus pertinente (A vérif)

#5
#Si elles sont complètement non corrélées, approprié de les inclure toutes les 2 car infos différentes et complémentaires
#Cela peut permettre d'obtenir une meilleure compréhension de la structure des données.

#6
num_comments = dataset[5]
num_shares = dataset[6]
num_likes = dataset[7]
num_loves = dataset[8]

var(num_comments)
var(num_shares)
var(num_likes)
var(num_loves)

#Variables ont des échelles diff, donc des variances diff
#Standardisation consiste à réduire les variables à une moyenne de 0 et une variance de 1
#permet de mettre toutes les variables sur la même échelle et de les comparer aux autres

#7
# Standardize the variables
dataset_scaled <- scale(dataset[,c("num_comments", "num_shares", "num_likes", "num_loves")])

# Perform PCA
pca <- prcomp(dataset_scaled, scale = TRUE)

# Analyze the output
pca
#garder ,um_comments et num_shares
summary(pca)
#proportion of variance à regarder pour pve

#Faut analyser les 2 premières composantes et les interpréter.

#8
# Calculate the variance explained by each component
pve <- pca$sdev^2 / sum(pca$sdev^2) * 100
pve

barplot(pve, xlab = "Component", ylab = "PVE", col = "blue")

# Calculate the cumulative PVE
cumulative_pve <- cumsum(pve)

# Plot the cumulative PVE
plot(1:length(cumulative_pve), cumulative_pve, type = "l", xlab = "Component", ylab = "Cumulative PVE", col = "red")
#Les 2 premières parce qu'elles expliquent mieux (83%) (Argumenter)

#9
biplot(pca, scores=TRUE, var.axes=TRUE, cex=0.5, col=c("blue","red"),xlabs=rownames(dataset)) #possible d'inverser les vecteurs ???
#Interpréter les résultats

#10
#R² est un indicateur de la qualité de la régression linéaire. Il mesure la proportion de variance de la variable dépendante (Y) qui est expliquée par les variables indépendantes (X1 et X2). Plus R² est proche de 1, plus la régression linéaire est bonne.
#La plage de valeurs que peut prendre R² est comprise entre 0 et 1.
#La relation entre R² et les coefficients de corrélation r1 et r2 est que R² est égal à la somme des carrés des coefficients de corrélation r1 et r2.

#11
corel <- cor(dataset[, c("num_shares", "num_comments", "num_likes", "num_loves", "num_wows", "num_hahas", "num_sads", "num_angrys")])
corel
#Semble être le plus corrélé avec num-loves mais à vérif au cas où

#12.1
linear <- lm(num_shares ~ num_loves, dataset)
linear
#Coefficient estimés ?

#12.2
#L'expression générale d'un intervalle de confiance 1-α pour le paramètre β1 est donnée par : β1 ± t(α/2,n-2) * s(epsilon) / sqrt(Somme des carrés de X)
#où t(α/2,n-2) est la valeur critique de la loi de Student pour un niveau de confiance 1-α et s(epsilon) est l'écart-type des erreurs
confint(linear, level = 0.95)
# Faut Interpréter les résultats

#12.3
# test the null hypothesis that the slope is zero
hyp <- t.test(residuals(linear))

# print the p-value
p_value <- hyp$p.value
p_value

#100% probable que l'hypothèse soit vraie. L'hypothèse que le slope est 0 ne peut pas être rejeter.

#12.4
#value of the coefficient of determination R²
summary(linear)$r.squared
#signiie que  67.24% de la variance de la variable réponse est expliquée par les variables prédictives dans le modèle.
#R² relativement élevé, ce qui indique que le modèle s'adapte bien aux données et que les variables prédictives sont en mesure d'expliquer une grande proportion de la variation de la variable réponse.
#prend pas en compte certains aspects comme gestion des erreursdonc R² = bon indicateur mais suffit pas à garantir que le modèle est approprié pour prédire le nombre de parts
        
#13
# load the leaps library
library(leaps)

# select the predictors with the highest correlation
high_cor_predictors <- names(which(abs(corel[,"num_shares"]) > 0.5))

# fit the reduced model with only the high correlation predictors
linear_reduced <- lm(num_shares ~ ., data=dataset[,high_cor_predictors])

# perform best subset selection with up to 3 predictors
subset_linear <- regsubsets(num_shares ~ ., data=dataset[,high_cor_predictors])

# summarize the results
summary(subset_linear)
names(subset_linear)

# extract the R^2 statistics for each model
rsq <- summary(subset_linear)$rsq

# plot the R^2 statistics versus the number of features
plot(1:5, rsq, xlab="Number of Features", ylab="R^2")

# extract the adjusted R^2 statistics for each model
adjrsq <- summary(subset_linear)$adjr2

# find the model with the highest adjusted R^2 value
best_model <- which.max(adjrsq)
best_model
